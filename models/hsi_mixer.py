# -*- coding: utf-8 -*-
"""
Created on Fri May 20 11:46:01 2022

@author: Blues
"""
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
import torch 
import torch.nn as nn
import numpy as np
from itertools import repeat
import collections.abc
import torch.nn.functional as F
from torchsummary import summary
import math
import warnings
#from Utils.operations import feedforward
import sys
def _ntuple(n):
    def parse(x):
        if isinstance(x, collections.abc.Iterable):
            return x
        return tuple(repeat(x, n))
    return parse


to_1tuple = _ntuple(1)
to_2tuple = _ntuple(2)
to_3tuple = _ntuple(3)
to_4tuple = _ntuple(4)
to_ntuple = _ntuple

def _no_grad_trunc_normal_(tensor, mean=0, std=1, a=-2., b=2.):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def torch_SAD(cen_idx, nei_idx):
    cen_idx = torch.sigmoid(cen_idx)
    nei_idx = torch.sigmoid(nei_idx)
    
    #cen_idx = F.normalize(cen_idx, dim=0)
    #nei_idx = F.normalize(nei_idx, dim=0)
    cos_alpha = cen_idx@nei_idx/(torch.sqrt(torch.sum(torch.pow(cen_idx, 2)))\
                     *torch.sqrt(torch.sum(torch.pow(nei_idx, 2))))

    print(cos_alpha)
    return torch.acos(cos_alpha)
'''
class hybrid_position_embedding(nn.Module):
    def __init__(self, keep_prob=0.9, block_size=3, beta=0.9):
        super(hybrid_position_embedding, self).__init__()
        self.patch_size=1
        self.beta = beta
        self.keep_prob = keep_prob
        self.block_size = block_size
        self.gelu = nn.GELU()
        self.softmax = nn.Softmax(dim=1)
    def normalize(self, input):
        min_c, max_c = input.min(1, keepdim=True)[0], input.max(1, keepdim=True)[0]
        input_norm = (input - min_c) / (max_c - min_c + 1e-8)
        return input_norm
    
    def cal_SAD(self,cen_idx, nei_idx):
        cos_alpha = cen_idx@nei_idx/(torch.sqrt(torch.sum(torch.pow(cen_idx, 2)))\
                     *torch.sqrt(torch.sum(torch.pow(nei_idx, 2))))
        #eps = 1e-6
        #cos_alpha = torch.where(cos_alpha < 1.0 + eps, 1.0 , cos_alpha)
        #cos_alpha = torch.where(cos_alpha < -1.0, -1.0 , cos_alpha)
        
        #cos_alpha = torch.tensor(cos_alpha).long()     
        return torch.acos(cos_alpha)
    
    def cal_SID(self,y_true, y_pred):
        #y_true = torch.where(torch.min(y_true) < 0, y_true - torch.min(y_true) + 1e-7 , y_true + 1e-7) 
        y_true = y_true + 1e-7
        y_pred = y_pred + 1e-7
        p_n = y_true / torch.sum(y_true, dim=-1, keepdims=True)
        q_n = y_pred / torch.sum(y_pred, dim=-1, keepdims=True)
        return torch.sum(p_n * torch.log(p_n / q_n)) + torch.sum(q_n * torch.log(q_n / p_n))
    
    def forward(self, input):
        input_norm = self.normalize(input).detach()
        B, C, P = input_norm.size()
        cen_idx = int((P-1) / 2)
        hybrid_matrix = torch.zeros((B, P))
        for i in range(B):
            for j in range(P):
                hybrid_matrix[i,j] = 1-(self.cal_SID(input_norm[i, :, cen_idx], input_norm[i, :, j]) \
                             *torch.tan(self.cal_SAD(input_norm[i, :, cen_idx], input_norm[i, :, j])))
        hybrid_matrix = hybrid_matrix.view(input.size()[0],1,input.size()[2]).to(device=input.device, dtype=input.dtype)
        Hmap = F.conv1d(hybrid_matrix,
                        torch.ones((1,1, self.patch_size)).to(device=input.device,
                                                             dtype=input.dtype),
                        padding=self.patch_size//2,
                        groups=1)
        Out = self.softmax(Hmap)
        return Out
'''
class hybrid_position_embedding(nn.Module):
    def __init__(self, keep_prob=0.9, block_size=3, beta=0.9):
        super(hybrid_position_embedding, self).__init__()
        self.patch_size=1
        self.beta = beta
        self.keep_prob = keep_prob
        self.block_size = block_size
        self.gelu = nn.GELU()
        self.softmax = nn.Softmax(dim=1)
    def normalize(self, input):
        min_c, max_c = input.min(1, keepdim=True)[0], input.max(1, keepdim=True)[0]
        input_norm = (input - min_c) / (max_c - min_c + 1e-8)
        return input_norm
    
    def cal_SAD(self,cen_idx, nei_idx):
        cos_alpha = cen_idx@nei_idx/(np.sqrt(np.sum(np.power(cen_idx, 2)))\
                     *np.sqrt(np.sum(np.power(nei_idx, 2))))
        eps = 1e-6
    
        if 1.0 < cos_alpha < 1.0 + eps:
                cos_alpha = 1.0
        elif -1.0 - eps < cos_alpha < -1.0:
                cos_alpha = -1.0
        return np.arccos(cos_alpha)
    
    def cal_SID(self,y_true, y_pred):
        #y_true = torch.where(torch.min(y_true) < 0, y_true - torch.min(y_true) + 1e-7 , y_true + 1e-7)    
        y_true = y_true + 1e-7
        y_pred = y_pred + 1e-7
        p_n = y_true / np.sum(y_true, axis=-1, keepdims=True)
        q_n = y_pred / np.sum(y_pred, axis=-1, keepdims=True)
        return np.sum(p_n * np.log(p_n / q_n)) + np.sum(q_n * np.log(q_n / p_n))
    
    def forward(self, input):
        input_norm = self.normalize(input).detach().cpu().numpy()
        #print(input_norm.shape)
        #B, C, P = input_norm.shape
        cen_idx = int((input_norm.shape[2]-1) / 2)
        hybrid_matrix = np.zeros([input_norm.shape[0], input_norm.shape[2]])
        for i in range(input_norm.shape[0]):
            for j in range(input_norm.shape[2]):
                hybrid_matrix[i,j] = 1-(self.cal_SID(input_norm[i, :, cen_idx], input_norm[i, :, j]) \
                *np.tan(self.cal_SAD(input_norm[i, :, cen_idx], input_norm[i, :, j])))
        
        hybrid_matrix = torch.from_numpy(hybrid_matrix).long().to(input.device,
                                                            dtype=input.dtype)
        hybrid_matrix = hybrid_matrix.view(input.size()[0],1,input.size()[2])
        Hmap = F.conv1d(hybrid_matrix,
                        torch.ones((1,1, self.patch_size)).to(device=input.device,
                                                             dtype=input.dtype),
                        padding=self.patch_size//2,
                        groups=1)
        Out = self.softmax(Hmap)
        #if not self.training or self.keep_prob == 1:
        #    return input
        '''
        ######################################################################
        gamma = (1. - self.keep_prob) / self.block_size ** 2
        for sh in input.shape[2:]:
            gamma *= sh / (sh - self.block_size + 1)
        M = torch.bernoulli(torch.ones_like(hybrid_matrix) * gamma)
        Hmap = F.conv1d(M,
                        torch.ones((1,1, self.patch_size)).to(device=input.device,
                                                             dtype=input.dtype),
                        padding=self.patch_size//2,
                        groups=1)
        Hmap = (Hmap < 1).to(device=input.device, dtype=input.dtype)
        input2 = hybrid_matrix * Hmap
        x_norm = self.normalize(input2)
        mask = (x_norm > self.beta).float()
        block_mask = 1 - (mask * x_norm)
        return hybrid_matrix*block_mask
        '''
        return Out


def measure_pos_embd(inputs):
    print(inputs)
    B, C, P = inputs.size()
    cen_idx = int((P-1) / 2)
    #print(cen_idx)
    spc_matrix = torch.zeros([B,P])
    #spa_matrix = torch.zeros_like(inputs)
    
    for i in range(B):
        for j in range(P):
            spc_matrix[i,j] = torch_SAD(inputs[i, :, cen_idx],inputs[i, :, j])
    #print(spc_matrix.size())
    print(spc_matrix)
    device = torch.device('cuda:0') 
    spc_matrix = spc_matrix.to(device)
    
    return spc_matrix
    
class Residual(nn.Module):
    def __init__(self, fn):
        super().__init__()
        self.fn = fn

    def forward(self, x):
        return self.fn(x) + x    

class Linear_projection(nn.Module):
    '''Linear projection with hybrid measurement pos embeddings for Neighboring HSI cuboids 
    '''
    def __init__(self, img_size=9, patch_size=3, in_chans=200, embed_dim=28):
        super(Linear_projection, self).__init__()
        input_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        #num_patches = (input_size[1] // patch_size[1]) * (input_size[0] // patch_size[0])
        self.kernel_size = (patch_size[0], patch_size[1])
        self.strides = (1, 1)
        self.gelu = nn.GELU()
        self.input_size = input_size
        self.patch_size = patch_size
        #self.num_patches = num_patches
        self.conv = nn.Conv2d(in_chans, int((in_chans-7)/2+1), kernel_size=self.kernel_size, stride=self.strides, padding=1)
        self.relu = nn.ReLU(inplace=True)
        #self.measure_pos_embd = measure_pos_embd
        self.hybrid = hybrid_position_embedding()
        self.proj = nn.Conv3d(1, embed_dim, kernel_size = (7, 1, 1), stride = (7, 1, 1))
    def forward(self, x):
        B, C, H, W = x.size()

        assert H == self.input_size[0] and W == self.input_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.input_size[0]}*{self.input_size[1]})."

        x = self.conv(x)
        
        Cout = x.size()[1]
        x = x.view(B,Cout,H*W)
        hybrid_embd = self.hybrid(x)
        #measure_pos = self.measure_pos_embd(x)
        #x = torch.cat([x, measure_pos.view(B,1,H*W)], dim=1)
        #print(x.size())
        #x = x + hybrid_embd
        x = torch.cat([x, hybrid_embd], dim=1)
        x = x.view(B,1,-1,H,W)
        x = self.proj(x)
        return x

def Spectral_Mixer_Layer(dim, depth, kernel_size=(7, 1, 1), patch_size=7):
    return nn.Sequential(
            *[nn.Sequential(
                Residual(nn.Sequential(
                    nn.Conv3d(dim, dim, kernel_size, groups=dim, padding=(int(kernel_size[0]/2), 0, 0)), 
                    nn.GELU(),
                    nn.BatchNorm3d(dim)
                )), 
                nn.Conv3d(dim, dim, kernel_size=1, padding=(0,0,0)),
                nn.GELU(),
                nn.BatchNorm3d(dim) 
            ) for i in range(depth)])

def Spatial_Mixer_Layer(dim, depth, kernel_size=(1, 7, 7), patch_size=7):
    return nn.Sequential(
            *[nn.Sequential(
                Residual(nn.Sequential(
                    #nn.ReplicationPad3d((kernel_size[1]-1, kernel_size[1]-1, kernel_size[2]-1, kernel_size[2]-1, 0, 0)),
                    #nn.Conv3d(dim, dim, kernel_size, groups=dim, padding=(0, int(kernel_size[1]/2), int(kernel_size[2]/2)), dilation=(1,1,1)),
                    nn.Conv3d(dim, dim, kernel_size, groups=dim, dilation=(1,2,2),padding=(0,2,2)),
                    nn.GELU(),
                    nn.BatchNorm3d(dim)
                )), 
                nn.Conv3d(dim, dim, kernel_size=1, padding=(0,0,0)),
                nn.GELU(),
                nn.BatchNorm3d(dim) 
            ) for i in range(depth)])

class Spectral_Spatial_Mixer_Block(nn.Module):
    '''For building Spectral Mixer Block
    '''
    def __init__(self, dim, depth, spec_kernel_size=(7, 1, 1), spa_kernel_size=(1, 7, 7)):
        super(Spectral_Spatial_Mixer_Block, self).__init__()
        self.spectral_mixer = Spectral_Mixer_Layer(dim, depth, spec_kernel_size)
        self.spatial_mixer = Spatial_Mixer_Layer(dim, depth, spa_kernel_size)
        #self.spc_spa_mixer = Mini_Spectral_Spatial_Mixer_Block(dim, depth, spec_kernel_size, spa_kernel_size)
    def forward(self, x):
        
        x = self.spectral_mixer(x)
        x = self.spatial_mixer(x)
        #x = self.spc_spa_mixer(x)
        return x
            
class HSI_Mixer_Net(nn.Module):
    '''Construct HSI_Mixer architecture
    '''
    def __init__(self, num_classes = 16, img_size=65, patch_size=13, depth=1, in_chans=200, embed_dim=13*13):
        super(HSI_Mixer_Net, self).__init__()
        self.linear_projection = Linear_projection(img_size, patch_size, in_chans, embed_dim)
        self.spc_spa_mixer = Spectral_Spatial_Mixer_Block(dim=embed_dim, depth=depth, spec_kernel_size=(7, 1, 1), spa_kernel_size=(1, 3, 3))
        self.pool = nn.AdaptiveAvgPool3d((1,1,1))
        #self.flat = torch.flatten()
        self.Linear = nn.Linear(embed_dim, num_classes)
    def forward(self, x):
        #print(x.size())
        n, c, h, w = x.size()
        x = self.linear_projection(x)
        x = self.spc_spa_mixer(x)
        x = self.pool(x)
        x = x.view(x.size()[0], -1)
        x = self.Linear(x)
        return x    
if __name__ == '__main__':
    
    input = torch.randn(16, 176, 9, 9)
    #input = torch.randn(1, 3, 224, 224)
    #net = HSI_Mixer_Net(num_classes = 16, img_size = 224, patch_size = 16, in_chans = 3, embed_dim=768)
    net = HSI_Mixer_Net(num_classes = 16, img_size = input.size()[3], patch_size = 3, in_chans = input.size()[1], embed_dim=28)    
    net.cuda()
    summary(net, (176,9,9))
    
    
    
    
    